{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 16:28:38.857974: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-21 16:28:38.976731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737473319.027645    6434 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737473319.041406    6434 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-21 16:28:39.156364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from engine.ner_detector import tokenize_evaluate_and_detect_NERs\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from engine.data import prepare_data_for_fine_tuning, read_data\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import copy\n",
    "import spacy\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available. Using GPU.\")\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        print(\"CUDA not available. Using CPU.\")\n",
    "        return \"cpu\"\n",
    "    \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dawid/studies/master-2/master24nlp/nlp-2024-fake'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "models = Path(\"output\")\n",
    "all_files = [path for path in models.rglob(\"model_final/model.safetensors\")]\n",
    "accuracies = []\n",
    "for file in all_files:\n",
    "    with open(file.parent.with_name(\"test_acc.json\"), \"r\") as f:\n",
    "        accuracies.append(float(f.readline().strip()))\n",
    "\n",
    "models = [(path.parts[2], path.parts[1], path.parts[3], path.parts[4]) for path in all_files]\n",
    "model_df = pd.DataFrame(models, columns=[\"model\", \"dataset\", \"training_type\", \"run\"])\n",
    "model_df[\"accuracy\"] = accuracies\n",
    "model_df = model_df.iloc[model_df.groupby([\"model\", \"dataset\", \"training_type\"])[\"accuracy\"].idxmax(), ]\n",
    "model_df = model_df[model_df[\"model\"] == \"roberta\"]\n",
    "model_df\n",
    "\n",
    "# model_df = pd.DataFrame(['ernie', 'coaid', 'masked', '1']).T\n",
    "# model_df.rename({0: 'model', 1: 'dataset', 2: 'training_type', 3: 'run'}, inplace=True, axis=1)\n",
    "# model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_relative_importance(pipeline, \n",
    "                                  test_dataset):\n",
    "    res2 = tokenize_evaluate_and_detect_NERs(pipeline, \n",
    "                                  test_dataset['text'], \n",
    "                                  spacy_model=\"en_core_web_lg\",\n",
    "                                  return_mappings_for_each_text=True)\n",
    "    ratios = []\n",
    "    for sentence in res2:\n",
    "        avg_per_imp = np.array(list(map(lambda x: abs(x[1]), filter(lambda z: z[2] == 'PERSON', sentence)))).mean()\n",
    "        avg_imp = np.array(list(map(lambda x: abs(x[1]), sentence))).mean()\n",
    "        ratios.append(avg_per_imp / avg_imp)\n",
    "        \n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_person_importance(res):\n",
    "    persons = list(map(lambda x: x[0], filter(lambda x: x[2] == 'PERSON', res)))\n",
    "    importances = list(map(lambda x: x[1], filter(lambda x: x[2] == 'PERSON', res)))\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    persons_unique = {}\n",
    "    per = ''\n",
    "\n",
    "    idx = 0\n",
    "    imp = 0\n",
    "    cnt = 0\n",
    "    while idx != len(persons):\n",
    "        if per == '':\n",
    "            per = persons[idx]\n",
    "            imp = importances[idx]\n",
    "            cnt = 1\n",
    "            idx += 1\n",
    "        elif persons[idx][:2] == '##':\n",
    "            per = per + persons[idx][2:]\n",
    "            imp += importances[idx]\n",
    "            cnt += 1\n",
    "            idx += 1\n",
    "        else:\n",
    "            if per not in persons_unique.keys():\n",
    "                persons_unique[per] = []\n",
    "            persons_unique[per].append(imp / cnt)\n",
    "            cnt = 0\n",
    "            per = ''\n",
    "            \n",
    "    new_persons = {}\n",
    "    for key in persons_unique:\n",
    "        \n",
    "        doc = nlp(key)\n",
    "        is_ok = False\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                is_ok = True\n",
    "                break\n",
    "        \n",
    "        if is_ok:\n",
    "            new_persons[key] = np.mean(persons_unique[key])\n",
    "        \n",
    "    return new_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_persons(persons_unique, negative = False, n = 5):\n",
    "    importance = list(persons_unique.values())\n",
    "    persons = list(persons_unique.keys())\n",
    "    importance = np.array(importance)\n",
    "    persons = np.array(persons)\n",
    "    if not negative:\n",
    "        importance = -importance\n",
    "    top_persons = persons[np.argsort(importance)[:n]]\n",
    "    return top_persons.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_out_to_vec(pipeline_out):\n",
    "    preds = []\n",
    "    for out in pipeline_out:\n",
    "        if out[0]['label'] == 'LABEL_1':\n",
    "            preds.append(out[0]['score'])\n",
    "        else:\n",
    "            preds.append(out[1]['score'])\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_random_person_words(sentence, persons):\n",
    "    found = set(sentence.lower().split()).intersection(persons)\n",
    "    if len(found) == 0:\n",
    "        return 'NOT EXIST'\n",
    "    else:\n",
    "        return list(found)[np.random.choice(len(found))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prediction(pred):\n",
    "    if pred[0][\"label\"] == \"LABEL_1\":\n",
    "        return pred[0][\"score\"]\n",
    "    else:\n",
    "        return pred[1][\"score\"]\n",
    "\n",
    "results = {}\n",
    "results_misc = {}\n",
    "\n",
    "for _, row in model_df.iterrows():\n",
    "    \n",
    "    results_misc['-'.join([row[\"dataset\"], row[\"model\"], row[\"training_type\"]])] = {}\n",
    "    \n",
    "    print(row[\"dataset\"])\n",
    "    model_path = Path(\"output\", row[\"dataset\"], row[\"model\"], row[\"training_type\"], row[\"run\"], \"model_final\", \"model.safetensors\")\n",
    "    model_id = \"roberta-base\" if row[\"model\"] == \"roberta\" else \"nghuyong/ernie-2.0-base-en\"\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, config=config\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    data = pd.read_csv(Path(\"data\", row[\"dataset\"], \"test.csv\"), header=0)\n",
    "    test_dataset = prepare_data_for_fine_tuning(data, tokenizer)\n",
    "    model.eval()\n",
    "    pipeline = TextClassificationPipeline(\n",
    "        model=model, tokenizer=tokenizer, top_k=2, device=device\n",
    "    )\n",
    "\n",
    "    if(device == \"cuda\"):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "        \n",
    "    ratios = get_person_relative_importance(pipeline, test_dataset)\n",
    "    results_misc['-'.join([row[\"dataset\"], row[\"model\"], row[\"training_type\"]])]['ratios'] = ratios\n",
    "    \n",
    "    \n",
    "    res = tokenize_evaluate_and_detect_NERs(pipeline, \n",
    "                                  test_dataset['text'], \n",
    "                                  spacy_model=\"en_core_web_lg\")\n",
    "    \n",
    "    person_importance_mapping = get_map_person_importance(res)\n",
    "    top_positive_persons = get_top_persons(person_importance_mapping, negative=False)\n",
    "    top_negative_persons = get_top_persons(person_importance_mapping, negative=True)\n",
    "    \n",
    "    orig_pred = pipeline_out_to_vec(pipeline(test_dataset[\"text\"]))\n",
    "    preds = orig_pred\n",
    "    \n",
    "    replacements = []\n",
    "    test_counterfactuals = test_dataset.to_pandas().copy()\n",
    "\n",
    "    for ix in test_counterfactuals.index:\n",
    "        top_per_idx = np.random.choice(10)\n",
    "        if preds[ix] > 0.5:\n",
    "            person_to_add = top_negative_persons[top_per_idx]\n",
    "        else:\n",
    "            person_to_add = top_positive_persons[top_per_idx]\n",
    "            \n",
    "        text = test_counterfactuals.loc[ix, [\"text\"]].get(0)\n",
    "        person_to_remove = find_random_person_words(text.lower(), person_importance_mapping.keys())\n",
    "        test_counterfactuals.loc[ix, [\"text\"]] = text.replace(person_to_remove, person_to_add)\n",
    "        \n",
    "        replacements.append((person_to_remove, person_to_add))\n",
    "            \n",
    "    \n",
    "    adv_pred = pipeline_out_to_vec(pipeline(test_counterfactuals[\"text\"].to_list()))\n",
    "    \n",
    "    \n",
    "    results['-'.join([row[\"dataset\"], row[\"model\"], row[\"training_type\"]])] = {\"orig_pred\": orig_pred, \"adv_pred\": adv_pred, \"replacements\": replacements}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "with open(\"results.json\", \"w\") as file:\n",
    "    json.dump(results, file)\n",
    "    \n",
    "with open('results_misc.pkl', 'wb') as file:\n",
    "    pkl.dump(results_misc, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
