{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: does `model` returns `[0 prob, 1 prob]` before softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "from engine.data import prepare_data_for_fine_tuning, read_data\n",
    "from engine.xai import FeatureAblationText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"roberta-base\"\n",
    "MODEL_PATH = \"output/checkpoint-2025/model.safetensors\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fifty-six percent decline in overall crime. A 73 percent decline in motor-vehicle theft. A 67 percent decline in robbery. A 66 percent decline in murder. This is way beyond what happened in the nation during this period of time.\tcrime\trudy-giuliani\tAttorney\tNew York\trepublican\t9\t11\t10\t7\t3\tNew York, N.Y.\n",
      "5122.json\tmostly-true\tIn 1958, there were 16 states in this country that prohibited -- prohibited -- an African-American and a Caucasian from being married.\tcivil-rights,gays-and-lesbians,marriage\tsheila-oliver\tAssemblywoman\tNew Jersey\tdemocrat\t0\t1\t1\t3\t0\ta news conference\n",
      "11191.json\ttrue\tSays Donald Trump has changed his mind on abortion.\tabortion\tcarly-fiorina\t\tCalifornia\trepublican\t5\t5\t4\t3\t2\tthe first Republican presidential debate\n",
      "10315.json\tmostly-true\tIt has been many years, if ever, since an inmate has completed his or her high school diploma while incarcerated in a state correctional facility for adults.\tcorrections-and-updates,criminal-justice,public-safety\tlc-buster-evans\tAssistant Commissioner\tGeorgia\tnone\t0\t0\t0\t1\t0\tin a press release\n",
      "1964.json\tmostly-true\tSays U.S. Rep. Michael McCaul is the sixth-richest person in Congress.\tcandidates-biography\tted-ankrum\tRetired\tTexas\tdemocrat\t0\t1\t0\t1\t0\ta speech\n",
      "624.json\ttrue\tMcCain is raising campaign cash with one of (Jack) Abramoff's closest business partners: scandal-plagued conservative activist Ralph Reed.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 790/790 [00:00<00:00, 4591.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test = read_data(\"data/split_raw/test.tsv\")\n",
    "# print(test)\n",
    "test_dataset = prepare_data_for_fine_tuning(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available. Using GPU.\")\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        print(\"CUDA not available. Using CPU.\")\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_ID)\n",
    "device = get_device()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH, config=config\n",
    ")\n",
    "model.eval()\n",
    "pipeline = TextClassificationPipeline(\n",
    "    model=model, tokenizer=tokenizer, top_k=2, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if(device == \"cuda\"):\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fifty-six percent decline in overall crime. A 73 percent decline in motor-vehicle theft. A 67 percent decline in robbery. A 66 percent decline in murder. This is way beyond what happened in the nation during this period of time.\tcrime\trudy-giuliani\tAttorney\tNew York\trepublican\t9\t11\t10\t7\t3\tNew York, N.Y.\n",
      "5122.json\tmostly-true\tIn 1958, there were 16 states in this country that prohibited -- prohibited -- an African-American and a Caucasian from being married.\tcivil-rights,gays-and-lesbians,marriage\tsheila-oliver\tAssemblywoman\tNew Jersey\tdemocrat\t0\t1\t1\t3\t0\ta news conference\n",
      "11191.json\ttrue\tSays Donald Trump has changed his mind on abortion.\tabortion\tcarly-fiorina\t\tCalifornia\trepublican\t5\t5\t4\t3\t2\tthe first Republican presidential debate\n",
      "10315.json\tmostly-true\tIt has been many years, if ever, since an inmate has completed his or her high school diploma while incarcerated in a state correctional facility for adults.\tcorrections-and-updates,criminal-justice,public-safety\tlc-buster-evans\tAssistant Commissioner\tGeorgia\tnone\t0\t0\t0\t1\t0\tin a press release\n",
      "1964.json\tmostly-true\tSays U.S. Rep. Michael McCaul is the sixth-richest person in Congress.\tcandidates-biography\tted-ankrum\tRetired\tTexas\tdemocrat\t0\t1\t0\t1\t0\ta speech\n",
      "624.json\ttrue\tMcCain is raising campaign cash with one of (Jack) Abramoff's closest business partners: scandal-plagued conservative activist Ralph Reed.\"\n",
      "tensor([[    0,   597, 22129,    12, 13664,   135,  2991,    11,  1374,  1846,\n",
      "             4,    83,  6521,   135,  2991,    11,  4243,    12, 17855, 11317,\n",
      "          5751,     4,    83,  5545,   135,  2991,    11,  6279,     4,    83,\n",
      "          5138,   135,  2991,    11,  1900,     4,   152,    16,   169,  1684,\n",
      "            99,  1102,    11,     5,  1226,   148,    42,   675,     9,    86,\n",
      "             4, 50117, 18608, 50117,   338,  1906,   219,    12, 15696,   922,\n",
      "         20909, 50117, 41339, 50117,  4030,   469, 50117, 12597, 40171,   260,\n",
      "         50117,   466, 50117,  1225, 50117,   698, 50117,   406, 50117,   246,\n",
      "         50117,  4030,   469,     6,   234,     4,   975,     4, 50121, 50118,\n",
      "           245, 21948,     4, 40962, 50117, 34663,    12, 29225, 50117,  1121,\n",
      "         23102,     6,    89,    58,   545,   982,    11,    42,   247,    14,\n",
      "          9986,   480,  9986,   480,    41,  1704,    12,  4310,     8,    10,\n",
      "         39787,    31,   145,  2997,     4, 50117, 37635,    12, 24733,     6,\n",
      "           571,  4113,    12,   463,    12,  1634, 46703,     6, 42855, 50117,\n",
      "          8877,  4882,    12,  1168,  8538, 50117, 45580,  7760, 50117,  4030,\n",
      "          3123, 50117, 12789, 22730, 50117,   288, 50117,   134, 50117,   134,\n",
      "         50117,   246, 50117,   288, 50117,   102,   340,  1019, 50121, 50118,\n",
      "          1225, 31080,     4, 40962, 50117, 29225, 50117,   104,  4113,   807,\n",
      "           140,    34,  1714,    39,  1508,    15,  6428,     4, 50117, 27275,\n",
      "         50117,  5901,   352,    12,   506,  7375,  1243, 50117, 50117, 26926,\n",
      "         50117, 12597, 40171,   260, 50117,   245, 50117,   245, 50117,   306,\n",
      "         50117,   246, 50117,   176, 50117,   627,    78,  1172,  1939,  2625,\n",
      "         50121, 50118, 18159,   996,     4, 40962, 50117, 34663,    12, 29225,\n",
      "         50117,   243,    34,    57,   171,   107,     6,   114,   655,     6,\n",
      "           187,    41, 12981,    34,  2121,    39,    50,    69,   239,   334,\n",
      "         26659,   150, 24593,    11,    10,   194, 30277,  2122,    13,  3362,\n",
      "             4, 50117, 36064,  2485,    12,   463,    12,   658, 19718,     6,\n",
      "         34928,    12, 28244,     6, 15110,    12, 29926, 50117, 45071,    12,\n",
      "         36543,    12,  3623,  1253, 50117, 46184,  4589, 50117, 39331, 50117,\n",
      "         39763, 50117,   288, 50117,   288, 50117,   288, 50117,   134, 50117,\n",
      "           288, 50117,   179,    10,  1228,   800, 50121, 50118, 45629,     4,\n",
      "         40962, 50117, 34663,    12, 29225, 50117,   104,  4113,   121,     4,\n",
      "           104,     4,  2825,     4,   988,  3409,  6695,    16,     5,  2958,\n",
      "            12,  4063, 20921,   621,    11,  1148,     4, 50117, 32503, 24143,\n",
      "            12,  5605, 10486, 50117,  5357,    12,  3153, 10904, 50117, 27814,\n",
      "          7651, 50117, 22456, 50117, 12789, 22730, 50117,   288, 50117,   134,\n",
      "         50117,   288, 50117,   134, 50117,   288, 50117,   102,  1901, 50121,\n",
      "         50118,   401,  1978,     4, 40962, 50117, 29225, 50117, 37730,  1851,\n",
      "            16,  3282,   637,  1055,    19,    65,     9,    36, 20907,    43,\n",
      "         28739,  1529,    18,  8099,   265,  2567,    35,  4220,    12,  2911,\n",
      "          1073,  6796,  3354,  6024, 10594,  7781,    72,     2]])\n"
     ]
    }
   ],
   "source": [
    "obs = test[\"text\"].tolist()[579]\n",
    "if(device == \"cuda\"):\n",
    "    obs_pt = pipeline.tokenizer(obs, return_tensors=\"pt\")['input_ids'].cuda()\n",
    "else:\n",
    "    obs_pt = pipeline.tokenizer(obs, return_tensors=\"pt\")['input_ids'].cpu()\n",
    "\n",
    "print(obs)\n",
    "print(obs_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(obs):\n",
    "    return model(obs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<engine.xai.FeatureAblationText object at 0x000001B36ADF0E90>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m attr \u001b[38;5;241m=\u001b[39m FeatureAblationText(forward)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(attr)\n\u001b[1;32m----> 3\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs_pt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m exp\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\nlp-2024-fake\\engine\\xai.py:29\u001b[0m, in \u001b[0;36mFeatureAblationText.get_attributions\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     27\u001b[0m exps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(obs))]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, ob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(obs):\n\u001b[1;32m---> 29\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     exps[idx] \u001b[38;5;241m=\u001b[39m exp\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(exps)\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\captum\\attr\\_core\\feature_ablation.py:351\u001b[0m, in \u001b[0;36mFeatureAblation.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, feature_mask, perturbations_per_eval, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[0;32m    332\u001b[0m     current_inputs,\n\u001b[0;32m    333\u001b[0m     current_add_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m#   non-agg mode:\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;66;03m#     (feature_perturbed * batch_size, *initial_eval.shape[1:])\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     modified_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strict_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_add_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[0;32m    359\u001b[0m         attr_progress\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\captum\\attr\\_core\\feature_ablation.py:599\u001b[0m, in \u001b[0;36mFeatureAblation._strict_run_forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strict_run_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m    A temp wrapper for global _run_forward util to force forward output\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m    type assertion & conversion.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m    Remove after the strict logic is supported by all attr classes\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 599\u001b[0m     forward_output \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(forward_output, Tensor):\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m forward_output\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\captum\\_utils\\common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(obs):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1318\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1318\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1330\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         output_attentions,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    519\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:447\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    439\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    446\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 447\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    457\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    368\u001b[0m )\n\u001b[1;32m--> 370\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    380\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "attr = FeatureAblationText(forward)\n",
    "print(attr)\n",
    "exp = attr.get_attributions([obs_pt])\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'F', 'ifty', '-', 'six', 'Ġpercent', 'Ġdecline', 'Ġin', 'Ġoverall', 'Ġcrime', '.', 'ĠA', 'Ġ73', 'Ġpercent', 'Ġdecline', 'Ġin', 'Ġmotor', '-', 'veh', 'icle', 'Ġtheft', '.', 'ĠA', 'Ġ67', 'Ġpercent', 'Ġdecline', 'Ġin', 'Ġrobbery', '.', 'ĠA', 'Ġ66', 'Ġpercent', 'Ġdecline', 'Ġin', 'Ġmurder', '.', 'ĠThis', 'Ġis', 'Ġway', 'Ġbeyond', 'Ġwhat', 'Ġhappened', 'Ġin', 'Ġthe', 'Ġnation', 'Ġduring', 'Ġthis', 'Ġperiod', 'Ġof', 'Ġtime', '.', 'ĉ', 'crime', 'ĉ', 'r', 'ud', 'y', '-', 'gi', 'ul', 'iani', 'ĉ', 'Attorney', 'ĉ', 'New', 'ĠYork', 'ĉ', 'rep', 'ublic', 'an', 'ĉ', '9', 'ĉ', '11', 'ĉ', '10', 'ĉ', '7', 'ĉ', '3', 'ĉ', 'New', 'ĠYork', ',', 'ĠN', '.', 'Y', '.', 'č', 'Ċ', '5', '122', '.', 'json', 'ĉ', 'mostly', '-', 'true', 'ĉ', 'In', 'Ġ1958', ',', 'Ġthere', 'Ġwere', 'Ġ16', 'Ġstates', 'Ġin', 'Ġthis', 'Ġcountry', 'Ġthat', 'Ġprohibited', 'Ġ--', 'Ġprohibited', 'Ġ--', 'Ġan', 'ĠAfrican', '-', 'American', 'Ġand', 'Ġa', 'ĠCaucasian', 'Ġfrom', 'Ġbeing', 'Ġmarried', '.', 'ĉ', 'civil', '-', 'rights', ',', 'g', 'ays', '-', 'and', '-', 'les', 'bians', ',', 'marriage', 'ĉ', 'she', 'ila', '-', 'ol', 'iver', 'ĉ', 'Assembly', 'woman', 'ĉ', 'New', 'ĠJersey', 'ĉ', 'dem', 'ocrat', 'ĉ', '0', 'ĉ', '1', 'ĉ', '1', 'ĉ', '3', 'ĉ', '0', 'ĉ', 'a', 'Ġnews', 'Ġconference', 'č', 'Ċ', '11', '191', '.', 'json', 'ĉ', 'true', 'ĉ', 'S', 'ays', 'ĠDonald', 'ĠTrump', 'Ġhas', 'Ġchanged', 'Ġhis', 'Ġmind', 'Ġon', 'Ġabortion', '.', 'ĉ', 'abortion', 'ĉ', 'car', 'ly', '-', 'f', 'ior', 'ina', 'ĉ', 'ĉ', 'California', 'ĉ', 'rep', 'ublic', 'an', 'ĉ', '5', 'ĉ', '5', 'ĉ', '4', 'ĉ', '3', 'ĉ', '2', 'ĉ', 'the', 'Ġfirst', 'ĠRepublican', 'Ġpresidential', 'Ġdebate', 'č', 'Ċ', '103', '15', '.', 'json', 'ĉ', 'mostly', '-', 'true', 'ĉ', 'It', 'Ġhas', 'Ġbeen', 'Ġmany', 'Ġyears', ',', 'Ġif', 'Ġever', ',', 'Ġsince', 'Ġan', 'Ġinmate', 'Ġhas', 'Ġcompleted', 'Ġhis', 'Ġor', 'Ġher', 'Ġhigh', 'Ġschool', 'Ġdiploma', 'Ġwhile', 'Ġincarcerated', 'Ġin', 'Ġa', 'Ġstate', 'Ġcorrectional', 'Ġfacility', 'Ġfor', 'Ġadults', '.', 'ĉ', 'correct', 'ions', '-', 'and', '-', 'up', 'dates', ',', 'criminal', '-', 'justice', ',', 'public', '-', 'safety', 'ĉ', 'lc', '-', 'buster', '-', 'ev', 'ans', 'ĉ', 'Assistant', 'ĠCommissioner', 'ĉ', 'Georgia', 'ĉ', 'none', 'ĉ', '0', 'ĉ', '0', 'ĉ', '0', 'ĉ', '1', 'ĉ', '0', 'ĉ', 'in', 'Ġa', 'Ġpress', 'Ġrelease', 'č', 'Ċ', '1964', '.', 'json', 'ĉ', 'mostly', '-', 'true', 'ĉ', 'S', 'ays', 'ĠU', '.', 'S', '.', 'ĠRep', '.', 'ĠMichael', 'ĠMcC', 'aul', 'Ġis', 'Ġthe', 'Ġsixth', '-', 'ric', 'hest', 'Ġperson', 'Ġin', 'ĠCongress', '.', 'ĉ', 'cand', 'idates', '-', 'bi', 'ography', 'ĉ', 'ted', '-', 'ank', 'rum', 'ĉ', 'Ret', 'ired', 'ĉ', 'Texas', 'ĉ', 'dem', 'ocrat', 'ĉ', '0', 'ĉ', '1', 'ĉ', '0', 'ĉ', '1', 'ĉ', '0', 'ĉ', 'a', 'Ġspeech', 'č', 'Ċ', '6', '24', '.', 'json', 'ĉ', 'true', 'ĉ', 'McC', 'ain', 'Ġis', 'Ġraising', 'Ġcampaign', 'Ġcash', 'Ġwith', 'Ġone', 'Ġof', 'Ġ(', 'Jack', ')', 'ĠAbram', 'off', \"'s\", 'Ġclosest', 'Ġbusiness', 'Ġpartners', ':', 'Ġscandal', '-', 'pl', 'ag', 'ued', 'Ġconservative', 'Ġactivist', 'ĠRalph', 'ĠReed', '.\"', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokens = pipeline.tokenizer.convert_ids_to_tokens(obs_pt[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in the text:\n",
      "Fifty-six percent (PERCENT) - Percentage, including \"%\"\n",
      "73 percent (PERCENT) - Percentage, including \"%\"\n",
      "67 percent (PERCENT) - Percentage, including \"%\"\n",
      "66 percent (PERCENT) - Percentage, including \"%\"\n",
      "New York (GPE) - Countries, cities, states\n",
      "republican (NORP) - Nationalities or religious or political groups\n",
      "9\t11\t10\t7\t3 (TIME) - Times smaller than a day\n",
      "New York (GPE) - Countries, cities, states\n",
      "N.Y. (GPE) - Countries, cities, states\n",
      "5122.json (CARDINAL) - Numerals that do not fall under another type\n",
      "1958 (DATE) - Absolute or relative dates or periods\n",
      "16 (CARDINAL) - Numerals that do not fall under another type\n",
      "African-American (NORP) - Nationalities or religious or political groups\n",
      "Caucasian (NORP) - Nationalities or religious or political groups\n",
      "sheila-oliver (ORG) - Companies, agencies, institutions, etc.\n",
      "Assemblywoman (GPE) - Countries, cities, states\n",
      "New Jersey (GPE) - Countries, cities, states\n",
      "democrat (NORP) - Nationalities or religious or political groups\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "1 (CARDINAL) - Numerals that do not fall under another type\n",
      "1 (CARDINAL) - Numerals that do not fall under another type\n",
      "3 (CARDINAL) - Numerals that do not fall under another type\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "Donald Trump (PERSON) - People, including fictional\n",
      "California (GPE) - Countries, cities, states\n",
      "republican (NORP) - Nationalities or religious or political groups\n",
      "5 (CARDINAL) - Numerals that do not fall under another type\n",
      "5 (CARDINAL) - Numerals that do not fall under another type\n",
      "first (ORDINAL) - \"first\", \"second\", etc.\n",
      "Republican (NORP) - Nationalities or religious or political groups\n",
      "many years (DATE) - Absolute or relative dates or periods\n",
      "Georgia (GPE) - Countries, cities, states\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "1 (CARDINAL) - Numerals that do not fall under another type\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "1964.json (CARDINAL) - Numerals that do not fall under another type\n",
      "U.S. (GPE) - Countries, cities, states\n",
      "Michael McCaul (PERSON) - People, including fictional\n",
      "sixth (ORDINAL) - \"first\", \"second\", etc.\n",
      "Congress (ORG) - Companies, agencies, institutions, etc.\n",
      "Retired\tTexas (PERSON) - People, including fictional\n",
      "democrat (NORP) - Nationalities or religious or political groups\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "1 (CARDINAL) - Numerals that do not fall under another type\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "1 (CARDINAL) - Numerals that do not fall under another type\n",
      "0 (CARDINAL) - Numerals that do not fall under another type\n",
      "McCain (PERSON) - People, including fictional\n",
      "one (CARDINAL) - Numerals that do not fall under another type\n",
      "Ralph Reed (PERSON) - People, including fictional\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = NER(obs)\n",
    "\n",
    "print(\"Entities in the text:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_}) - {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens and Their NER Information: \n",
      "\n",
      "Token: Fifty, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: -, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: six, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: percent, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: decline, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: overall, Not part of any entity.\n",
      "Token: crime, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: A, Not part of any entity.\n",
      "Token: 73, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: percent, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: decline, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: motor, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: vehicle, Not part of any entity.\n",
      "Token: theft, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: A, Not part of any entity.\n",
      "Token: 67, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: percent, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: decline, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: robbery, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: A, Not part of any entity.\n",
      "Token: 66, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: percent, Entity Type: PERCENT (Percentage, including \"%\")\n",
      "Token: decline, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: murder, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: This, Not part of any entity.\n",
      "Token: is, Not part of any entity.\n",
      "Token: way, Not part of any entity.\n",
      "Token: beyond, Not part of any entity.\n",
      "Token: what, Not part of any entity.\n",
      "Token: happened, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: the, Not part of any entity.\n",
      "Token: nation, Not part of any entity.\n",
      "Token: during, Not part of any entity.\n",
      "Token: this, Not part of any entity.\n",
      "Token: period, Not part of any entity.\n",
      "Token: of, Not part of any entity.\n",
      "Token: time, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: crime, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: rudy, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: giuliani, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: Attorney, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: New, Entity Type: GPE (Countries, cities, states)\n",
      "Token: York, Entity Type: GPE (Countries, cities, states)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: republican, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 9, Entity Type: TIME (Times smaller than a day)\n",
      "Token: \t, Entity Type: TIME (Times smaller than a day)\n",
      "Token: 11, Entity Type: TIME (Times smaller than a day)\n",
      "Token: \t, Entity Type: TIME (Times smaller than a day)\n",
      "Token: 10, Entity Type: TIME (Times smaller than a day)\n",
      "Token: \t, Entity Type: TIME (Times smaller than a day)\n",
      "Token: 7, Entity Type: TIME (Times smaller than a day)\n",
      "Token: \t, Entity Type: TIME (Times smaller than a day)\n",
      "Token: 3, Entity Type: TIME (Times smaller than a day)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: New, Entity Type: GPE (Countries, cities, states)\n",
      "Token: York, Entity Type: GPE (Countries, cities, states)\n",
      "Token: ,, Not part of any entity.\n",
      "Token: N.Y., Entity Type: GPE (Countries, cities, states)\n",
      "Token: \n",
      ", Not part of any entity.\n",
      "Token: 5122.json, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: mostly, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: true, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: In, Not part of any entity.\n",
      "Token: 1958, Entity Type: DATE (Absolute or relative dates or periods)\n",
      "Token: ,, Not part of any entity.\n",
      "Token: there, Not part of any entity.\n",
      "Token: were, Not part of any entity.\n",
      "Token: 16, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: states, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: this, Not part of any entity.\n",
      "Token: country, Not part of any entity.\n",
      "Token: that, Not part of any entity.\n",
      "Token: prohibited, Not part of any entity.\n",
      "Token: --, Not part of any entity.\n",
      "Token: prohibited, Not part of any entity.\n",
      "Token: --, Not part of any entity.\n",
      "Token: an, Not part of any entity.\n",
      "Token: African, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: -, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: American, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: and, Not part of any entity.\n",
      "Token: a, Not part of any entity.\n",
      "Token: Caucasian, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: from, Not part of any entity.\n",
      "Token: being, Not part of any entity.\n",
      "Token: married, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: civil, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: rights, Not part of any entity.\n",
      "Token: ,, Not part of any entity.\n",
      "Token: gays, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: and, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: lesbians, Not part of any entity.\n",
      "Token: ,, Not part of any entity.\n",
      "Token: marriage, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: sheila, Entity Type: ORG (Companies, agencies, institutions, etc.)\n",
      "Token: -, Entity Type: ORG (Companies, agencies, institutions, etc.)\n",
      "Token: oliver, Entity Type: ORG (Companies, agencies, institutions, etc.)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: Assemblywoman, Entity Type: GPE (Countries, cities, states)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: New, Entity Type: GPE (Countries, cities, states)\n",
      "Token: Jersey, Entity Type: GPE (Countries, cities, states)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: democrat, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 1, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 1, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 3, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: a, Not part of any entity.\n",
      "Token: news, Not part of any entity.\n",
      "Token: conference, Not part of any entity.\n",
      "Token: \n",
      ", Not part of any entity.\n",
      "Token: 11191.json, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: true, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: Says, Not part of any entity.\n",
      "Token: Donald, Entity Type: PERSON (People, including fictional)\n",
      "Token: Trump, Entity Type: PERSON (People, including fictional)\n",
      "Token: has, Not part of any entity.\n",
      "Token: changed, Not part of any entity.\n",
      "Token: his, Not part of any entity.\n",
      "Token: mind, Not part of any entity.\n",
      "Token: on, Not part of any entity.\n",
      "Token: abortion, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: abortion, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: carly, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: fiorina, Not part of any entity.\n",
      "Token: \t\t, Not part of any entity.\n",
      "Token: California, Entity Type: GPE (Countries, cities, states)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: republican, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 5, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 5, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 4, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 3, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 2, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: the, Not part of any entity.\n",
      "Token: first, Entity Type: ORDINAL (\"first\", \"second\", etc.)\n",
      "Token: Republican, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: presidential, Not part of any entity.\n",
      "Token: debate, Not part of any entity.\n",
      "Token: \n",
      ", Not part of any entity.\n",
      "Token: 10315.json, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: mostly, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: true, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: It, Not part of any entity.\n",
      "Token: has, Not part of any entity.\n",
      "Token: been, Not part of any entity.\n",
      "Token: many, Entity Type: DATE (Absolute or relative dates or periods)\n",
      "Token: years, Entity Type: DATE (Absolute or relative dates or periods)\n",
      "Token: ,, Not part of any entity.\n",
      "Token: if, Not part of any entity.\n",
      "Token: ever, Not part of any entity.\n",
      "Token: ,, Not part of any entity.\n",
      "Token: since, Not part of any entity.\n",
      "Token: an, Not part of any entity.\n",
      "Token: inmate, Not part of any entity.\n",
      "Token: has, Not part of any entity.\n",
      "Token: completed, Not part of any entity.\n",
      "Token: his, Not part of any entity.\n",
      "Token: or, Not part of any entity.\n",
      "Token: her, Not part of any entity.\n",
      "Token: high, Not part of any entity.\n",
      "Token: school, Not part of any entity.\n",
      "Token: diploma, Not part of any entity.\n",
      "Token: while, Not part of any entity.\n",
      "Token: incarcerated, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: a, Not part of any entity.\n",
      "Token: state, Not part of any entity.\n",
      "Token: correctional, Not part of any entity.\n",
      "Token: facility, Not part of any entity.\n",
      "Token: for, Not part of any entity.\n",
      "Token: adults, Not part of any entity.\n",
      "Token: ., Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: corrections, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: and, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: updates, Not part of any entity.\n",
      "Token: ,, Not part of any entity.\n",
      "Token: criminal, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: justice, Not part of any entity.\n",
      "Token: ,, Not part of any entity.\n",
      "Token: public, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: safety, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: lc, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: buster, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: evans, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: Assistant, Not part of any entity.\n",
      "Token: Commissioner, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: Georgia, Entity Type: GPE (Countries, cities, states)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: none, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 1, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: a, Not part of any entity.\n",
      "Token: press, Not part of any entity.\n",
      "Token: release, Not part of any entity.\n",
      "Token: \n",
      ", Not part of any entity.\n",
      "Token: 1964.json, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: mostly, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: true, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: Says, Not part of any entity.\n",
      "Token: U.S., Entity Type: GPE (Countries, cities, states)\n",
      "Token: Rep., Not part of any entity.\n",
      "Token: Michael, Entity Type: PERSON (People, including fictional)\n",
      "Token: McCaul, Entity Type: PERSON (People, including fictional)\n",
      "Token: is, Not part of any entity.\n",
      "Token: the, Not part of any entity.\n",
      "Token: sixth, Entity Type: ORDINAL (\"first\", \"second\", etc.)\n",
      "Token: -, Not part of any entity.\n",
      "Token: richest, Not part of any entity.\n",
      "Token: person, Not part of any entity.\n",
      "Token: in, Not part of any entity.\n",
      "Token: Congress, Entity Type: ORG (Companies, agencies, institutions, etc.)\n",
      "Token: ., Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: candidates, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: biography, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: ted, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: ankrum, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: Retired, Entity Type: PERSON (People, including fictional)\n",
      "Token: \t, Entity Type: PERSON (People, including fictional)\n",
      "Token: Texas, Entity Type: PERSON (People, including fictional)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: democrat, Entity Type: NORP (Nationalities or religious or political groups)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 1, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 1, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: 0, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: \t, Not part of any entity.\n",
      "Token: a, Not part of any entity.\n",
      "Token: speech, Not part of any entity.\n",
      "Token: \n",
      ", Not part of any entity.\n",
      "Token: 624.json, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: true, Not part of any entity.\n",
      "Token: \t, Not part of any entity.\n",
      "Token: McCain, Entity Type: PERSON (People, including fictional)\n",
      "Token: is, Not part of any entity.\n",
      "Token: raising, Not part of any entity.\n",
      "Token: campaign, Not part of any entity.\n",
      "Token: cash, Not part of any entity.\n",
      "Token: with, Not part of any entity.\n",
      "Token: one, Entity Type: CARDINAL (Numerals that do not fall under another type)\n",
      "Token: of, Not part of any entity.\n",
      "Token: (, Not part of any entity.\n",
      "Token: Jack, Not part of any entity.\n",
      "Token: ), Not part of any entity.\n",
      "Token: Abramoff, Not part of any entity.\n",
      "Token: 's, Not part of any entity.\n",
      "Token: closest, Not part of any entity.\n",
      "Token: business, Not part of any entity.\n",
      "Token: partners, Not part of any entity.\n",
      "Token: :, Not part of any entity.\n",
      "Token: scandal, Not part of any entity.\n",
      "Token: -, Not part of any entity.\n",
      "Token: plagued, Not part of any entity.\n",
      "Token: conservative, Not part of any entity.\n",
      "Token: activist, Not part of any entity.\n",
      "Token: Ralph, Entity Type: PERSON (People, including fictional)\n",
      "Token: Reed, Entity Type: PERSON (People, including fictional)\n",
      "Token: ., Not part of any entity.\n",
      "Token: \", Not part of any entity.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTokens and Their NER Information: \\n\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.ent_type_:\n",
    "        print(f\"Token: {token.text}, Entity Type: {token.ent_type_} ({spacy.explain(token.ent_type_)})\")\n",
    "    else:\n",
    "        print(f\"Token: {token.text}, Not part of any entity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "['F', 'ifty', '-', 'six', 'percent', 'decline', 'in', 'overall', 'crime', '.', 'A', '73', 'percent', 'decline', 'in', 'motor', '-', 'veh', 'icle', 'theft', '.', 'A', '67', 'percent', 'decline', 'in', 'robbery', '.', 'A', '66', 'percent', 'decline', 'in', 'murder', '.', 'This', 'is', 'way', 'beyond', 'what', 'happened', 'in', 'the', 'nation', 'during', 'this', 'period', 'of', 'time', '.', 'ĉ', 'crime', 'ĉ', 'r', 'ud', 'y', '-', 'gi', 'ul', 'iani', 'ĉ', 'Attorney', 'ĉ', 'New', 'York', 'ĉ', 'rep', 'ublic', 'an', 'ĉ', '9', 'ĉ', '11', 'ĉ', '10', 'ĉ', '7', 'ĉ', '3', 'ĉ', 'New', 'York', ',', 'N', '.', 'Y', '.', 'č', 'Ċ', '5', '122', '.', 'json', 'ĉ', 'mostly', '-', 'true', 'ĉ', 'In', '1958', ',', 'there', 'were', '16', 'states', 'in', 'this', 'country', 'that', 'prohibited', '--', 'prohibited', '--', 'an', 'African', '-', 'American', 'and', 'a', 'Caucasian', 'from', 'being', 'married', '.', 'ĉ', 'civil', '-', 'rights', ',', 'g', 'ays', '-', 'and', '-', 'les', 'bians', ',', 'marriage', 'ĉ', 'she', 'ila', '-', 'ol', 'iver', 'ĉ', 'Assembly', 'woman', 'ĉ', 'New', 'Jersey', 'ĉ', 'dem', 'ocrat', 'ĉ', '0', 'ĉ', '1', 'ĉ', '1', 'ĉ', '3', 'ĉ', '0', 'ĉ', 'a', 'news', 'conference', 'č', 'Ċ', '11', '191', '.', 'json', 'ĉ', 'true', 'ĉ', 'S', 'ays', 'Donald', 'Trump', 'has', 'changed', 'his', 'mind', 'on', 'abortion', '.', 'ĉ', 'abortion', 'ĉ', 'car', 'ly', '-', 'f', 'ior', 'ina', 'ĉ', 'ĉ', 'California', 'ĉ', 'rep', 'ublic', 'an', 'ĉ', '5', 'ĉ', '5', 'ĉ', '4', 'ĉ', '3', 'ĉ', '2', 'ĉ', 'the', 'first', 'Republican', 'presidential', 'debate', 'č', 'Ċ', '103', '15', '.', 'json', 'ĉ', 'mostly', '-', 'true', 'ĉ', 'It', 'has', 'been', 'many', 'years', ',', 'if', 'ever', ',', 'since', 'an', 'inmate', 'has', 'completed', 'his', 'or', 'her', 'high', 'school', 'diploma', 'while', 'incarcerated', 'in', 'a', 'state', 'correctional', 'facility', 'for', 'adults', '.', 'ĉ', 'correct', 'ions', '-', 'and', '-', 'up', 'dates', ',', 'criminal', '-', 'justice', ',', 'public', '-', 'safety', 'ĉ', 'lc', '-', 'buster', '-', 'ev', 'ans', 'ĉ', 'Assistant', 'Commissioner', 'ĉ', 'Georgia', 'ĉ', 'none', 'ĉ', '0', 'ĉ', '0', 'ĉ', '0', 'ĉ', '1', 'ĉ', '0', 'ĉ', 'in', 'a', 'press', 'release', 'č', 'Ċ', '1964', '.', 'json', 'ĉ', 'mostly', '-', 'true', 'ĉ', 'S', 'ays', 'U', '.', 'S', '.', 'Rep', '.', 'Michael', 'McC', 'aul', 'is', 'the', 'sixth', '-', 'ric', 'hest', 'person', 'in', 'Congress', '.', 'ĉ', 'cand', 'idates', '-', 'bi', 'ography', 'ĉ', 'ted', '-', 'ank', 'rum', 'ĉ', 'Ret', 'ired', 'ĉ', 'Texas', 'ĉ', 'dem', 'ocrat', 'ĉ', '0', 'ĉ', '1', 'ĉ', '0', 'ĉ', '1', 'ĉ', '0', 'ĉ', 'a', 'speech', 'č', 'Ċ', '6', '24', '.', 'json', 'ĉ', 'true', 'ĉ', 'McC', 'ain', 'is', 'raising', 'campaign', 'cash', 'with', 'one', 'of', '(', 'Jack', ')', 'Abram', 'off', \"'s\", 'closest', 'business', 'partners', ':', 'scandal', '-', 'pl', 'ag', 'ued', 'conservative', 'activist', 'Ralph', 'Reed', '.\"']\n",
      "tensor([[[ 0.0000e+00,  4.9621e-06,  8.8066e-06,  3.8594e-06,  7.4357e-06,\n",
      "           1.0684e-05,  1.0982e-05,  7.3612e-06,  9.5218e-06,  4.0531e-06,\n",
      "           4.0382e-06,  4.1127e-06,  3.5316e-06,  1.0267e-05,  1.1295e-05,\n",
      "           8.1062e-06,  4.1127e-06,  6.2287e-06,  1.0163e-05,  1.0222e-05,\n",
      "           2.2799e-06,  4.5598e-06,  4.2915e-06,  2.1160e-06,  8.4639e-06,\n",
      "           9.8199e-06,  5.2899e-06,  1.5199e-06,  2.6524e-06,  2.3842e-06,\n",
      "           1.7881e-07,  5.2899e-06,  5.3346e-06,  5.0664e-07, -3.5316e-06,\n",
      "          -2.3991e-06,  2.8312e-07, -5.9605e-07, -3.4273e-07,  2.1309e-06,\n",
      "          -4.1723e-07, -1.4901e-07,  1.5646e-06, -1.7881e-06,  2.2352e-07,\n",
      "           2.9057e-06,  1.6242e-06,  6.5714e-06,  4.1276e-06,  5.6922e-06,\n",
      "           2.8908e-06,  1.1623e-06],\n",
      "         [ 0.0000e+00,  2.0303e-07,  1.2014e-06, -1.3970e-07,  5.7742e-08,\n",
      "           1.2778e-06,  1.4901e-06,  6.3702e-07,  2.1160e-06, -1.7881e-07,\n",
      "          -1.3039e-07, -7.2643e-08, -5.3644e-07,  1.7211e-06,  2.0731e-06,\n",
      "           1.5181e-06,  1.8068e-07,  7.9349e-07,  2.4065e-06,  2.5015e-06,\n",
      "          -5.9232e-07,  1.2107e-07,  2.3283e-07, -4.6194e-07,  1.6913e-06,\n",
      "           1.7472e-06,  7.6368e-07, -1.1157e-06, -2.5332e-07, -4.1910e-07,\n",
      "          -8.2701e-07,  9.7975e-07,  7.3388e-07, -3.7998e-07, -2.3991e-06,\n",
      "          -1.2554e-06, -7.8417e-07, -2.7008e-07, -4.3586e-07,  2.6822e-07,\n",
      "          -5.0850e-07, -2.8312e-07,  2.3283e-07, -6.9663e-07, -5.4762e-07,\n",
      "           7.3202e-07,  1.6764e-07,  2.3488e-06,  1.6037e-06,  1.8161e-06,\n",
      "           1.1660e-06,  5.1223e-07]]])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "\n",
    "tokens_clear = [s.replace(\"Ġ\", \"\") for s in tokens]\n",
    "tokens_clear = tokens_clear[1:len(tokens_clear)-1]\n",
    "print(tokens_clear)\n",
    "\n",
    "print(exp)\n",
    "print(len(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.tokens_aggregate import TokenAggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 52 is out of bounds for dimension 2 with size 52",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m aggregate_list \u001b[38;5;241m=\u001b[39m \u001b[43mTokenAggregate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_aggregate_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokens_clear\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\1234o\\Studies\\Sem2\\NLP\\nlp-2024-fake\\engine\\tokens_aggregate.py:70\u001b[0m, in \u001b[0;36mTokenAggregate.generate_aggregate_list\u001b[1;34m(doc, exp_tensor, tokens_clear, tokens_dirty)\u001b[0m\n\u001b[0;32m     68\u001b[0m     clean_tokens_for_current_spacy_tokens\u001b[38;5;241m.\u001b[39mappend(tokens_clear[current_clear_token_id])\n\u001b[0;32m     69\u001b[0m     dirty_tokens_for_current_spacy_tokens\u001b[38;5;241m.\u001b[39mappend(tokens_dirty[current_clear_token_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 70\u001b[0m     model_token_exp\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(\u001b[43mexp_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcurrent_clear_token_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     71\u001b[0m     current_clear_token_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 52 is out of bounds for dimension 2 with size 52"
     ]
    }
   ],
   "source": [
    "aggregate_list = TokenAggregate.generate_aggregate_list(doc,exp,tokens_clear,tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregate nr: 0\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['Fifty']\n",
      "Our model clean: ['F', 'ifty']\n",
      "Our model dirty: ['F', 'ifty']\n",
      "model exp: [4.9620866775512695e-06, 8.806586265563965e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 1\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['-']\n",
      "Our model clean: ['-']\n",
      "Our model dirty: ['-']\n",
      "model exp: [3.859400749206543e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 2\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['six']\n",
      "Our model clean: ['six']\n",
      "Our model dirty: ['six']\n",
      "model exp: [7.4356794357299805e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 3\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['percent']\n",
      "Our model clean: ['percent']\n",
      "Our model dirty: ['Ġpercent']\n",
      "model exp: [1.068413257598877e-05]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 4\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['decline']\n",
      "Our model clean: ['decline']\n",
      "Our model dirty: ['Ġdecline']\n",
      "model exp: [1.0982155799865723e-05]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 5\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['in']\n",
      "Our model clean: ['in']\n",
      "Our model dirty: ['Ġin']\n",
      "model exp: [7.361173629760742e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 6\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['overall']\n",
      "Our model clean: ['overall']\n",
      "Our model dirty: ['Ġoverall']\n",
      "model exp: [9.521842002868652e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 7\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['crime']\n",
      "Our model clean: ['crime']\n",
      "Our model dirty: ['Ġcrime']\n",
      "model exp: [4.0531158447265625e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 8\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['.']\n",
      "Our model clean: ['.']\n",
      "Our model dirty: ['.']\n",
      "model exp: [4.038214683532715e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 9\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['A']\n",
      "Our model clean: ['A']\n",
      "Our model dirty: ['ĠA']\n",
      "model exp: [4.112720489501953e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 10\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['73']\n",
      "Our model clean: ['73']\n",
      "Our model dirty: ['Ġ73']\n",
      "model exp: [3.5315752029418945e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 11\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['percent']\n",
      "Our model clean: ['percent']\n",
      "Our model dirty: ['Ġpercent']\n",
      "model exp: [1.0266900062561035e-05]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 12\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['decline']\n",
      "Our model clean: ['decline']\n",
      "Our model dirty: ['Ġdecline']\n",
      "model exp: [1.1295080184936523e-05]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 13\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['in']\n",
      "Our model clean: ['in']\n",
      "Our model dirty: ['Ġin']\n",
      "model exp: [8.106231689453125e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 14\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['motor']\n",
      "Our model clean: ['motor']\n",
      "Our model dirty: ['Ġmotor']\n",
      "model exp: [4.112720489501953e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 15\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['-']\n",
      "Our model clean: ['-']\n",
      "Our model dirty: ['-']\n",
      "model exp: [6.22868537902832e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 16\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['vehicle']\n",
      "Our model clean: ['veh', 'icle']\n",
      "Our model dirty: ['veh', 'icle']\n",
      "model exp: [1.0162591934204102e-05, 1.0222196578979492e-05]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 17\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['theft']\n",
      "Our model clean: ['theft']\n",
      "Our model dirty: ['Ġtheft']\n",
      "model exp: [2.2798776626586914e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 18\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['.']\n",
      "Our model clean: ['.']\n",
      "Our model dirty: ['.']\n",
      "model exp: [4.559755325317383e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 19\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['A']\n",
      "Our model clean: ['A']\n",
      "Our model dirty: ['ĠA']\n",
      "model exp: [4.291534423828125e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 20\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['67']\n",
      "Our model clean: ['67']\n",
      "Our model dirty: ['Ġ67']\n",
      "model exp: [2.115964889526367e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 21\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['percent']\n",
      "Our model clean: ['percent']\n",
      "Our model dirty: ['Ġpercent']\n",
      "model exp: [8.463859558105469e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 22\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['decline']\n",
      "Our model clean: ['decline']\n",
      "Our model dirty: ['Ġdecline']\n",
      "model exp: [9.819865226745605e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 23\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['in']\n",
      "Our model clean: ['in']\n",
      "Our model dirty: ['Ġin']\n",
      "model exp: [5.289912223815918e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 24\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['robbery']\n",
      "Our model clean: ['robbery']\n",
      "Our model dirty: ['Ġrobbery']\n",
      "model exp: [1.519918441772461e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 25\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['.']\n",
      "Our model clean: ['.']\n",
      "Our model dirty: ['.']\n",
      "model exp: [2.652406692504883e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 26\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['A']\n",
      "Our model clean: ['A']\n",
      "Our model dirty: ['ĠA']\n",
      "model exp: [2.384185791015625e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 27\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['66']\n",
      "Our model clean: ['66']\n",
      "Our model dirty: ['Ġ66']\n",
      "model exp: [1.7881393432617188e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 28\n",
      "Is spacy NER: ['PERCENT']\n",
      "spacy token: ['percent']\n",
      "Our model clean: ['percent']\n",
      "Our model dirty: ['Ġpercent']\n",
      "model exp: [5.289912223815918e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 29\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['decline']\n",
      "Our model clean: ['decline']\n",
      "Our model dirty: ['Ġdecline']\n",
      "model exp: [5.334615707397461e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 30\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['in']\n",
      "Our model clean: ['in']\n",
      "Our model dirty: ['Ġin']\n",
      "model exp: [5.066394805908203e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 31\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['murder']\n",
      "Our model clean: ['murder']\n",
      "Our model dirty: ['Ġmurder']\n",
      "model exp: [-3.5315752029418945e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 32\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['.']\n",
      "Our model clean: ['.']\n",
      "Our model dirty: ['.']\n",
      "model exp: [-2.3990869522094727e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 33\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['This']\n",
      "Our model clean: ['This']\n",
      "Our model dirty: ['ĠThis']\n",
      "model exp: [2.8312206268310547e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 34\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['is']\n",
      "Our model clean: ['is']\n",
      "Our model dirty: ['Ġis']\n",
      "model exp: [-5.960464477539062e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 35\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['way']\n",
      "Our model clean: ['way']\n",
      "Our model dirty: ['Ġway']\n",
      "model exp: [-3.427267074584961e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 36\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['beyond']\n",
      "Our model clean: ['beyond']\n",
      "Our model dirty: ['Ġbeyond']\n",
      "model exp: [2.130866050720215e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 37\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['what']\n",
      "Our model clean: ['what']\n",
      "Our model dirty: ['Ġwhat']\n",
      "model exp: [-4.172325134277344e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 38\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['happened']\n",
      "Our model clean: ['happened']\n",
      "Our model dirty: ['Ġhappened']\n",
      "model exp: [-1.4901161193847656e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 39\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['in']\n",
      "Our model clean: ['in']\n",
      "Our model dirty: ['Ġin']\n",
      "model exp: [1.564621925354004e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 40\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['the']\n",
      "Our model clean: ['the']\n",
      "Our model dirty: ['Ġthe']\n",
      "model exp: [-1.7881393432617188e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 41\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['nation']\n",
      "Our model clean: ['nation']\n",
      "Our model dirty: ['Ġnation']\n",
      "model exp: [2.2351741790771484e-07]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 42\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['during']\n",
      "Our model clean: ['during']\n",
      "Our model dirty: ['Ġduring']\n",
      "model exp: [2.905726432800293e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 43\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['this']\n",
      "Our model clean: ['this']\n",
      "Our model dirty: ['Ġthis']\n",
      "model exp: [1.6242265701293945e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 44\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['period']\n",
      "Our model clean: ['period']\n",
      "Our model dirty: ['Ġperiod']\n",
      "model exp: [6.571412086486816e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 45\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['of']\n",
      "Our model clean: ['of']\n",
      "Our model dirty: ['Ġof']\n",
      "model exp: [4.127621650695801e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 46\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['time']\n",
      "Our model clean: ['time']\n",
      "Our model dirty: ['Ġtime']\n",
      "model exp: [5.692243576049805e-06]\n",
      "\n",
      "\n",
      "\n",
      "Agregate nr: 47\n",
      "Is spacy NER: ['']\n",
      "spacy token: ['.']\n",
      "Our model clean: ['.']\n",
      "Our model dirty: ['.']\n",
      "model exp: [2.8908252716064453e-06]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for aggregate in aggregate_list:\n",
    "    print(f\"Agregate nr: {i}\")\n",
    "    print(aggregate)\n",
    "    print(\"\\n\")\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fifty-six percent decline in overall crime. A 73 percent decline in motor-vehicle theft. A 67 percent decline in robbery. A 66 percent decline in murder. This is way beyond what happened in the nation during this period of time.\tcrime\trudy-giuliani\tAttorney\tNew York\trepublican\t9\t11\t10\t7\t3\tNew York, N.Y.\n",
      "5122.json\tmostly-true\tIn 1958, there were 16 states in this country that prohibited -- prohibited -- an African-American and a Caucasian from being married.\tcivil-rights,gays-and-lesbians,marriage\tsheila-oliver\tAssemblywoman\tNew Jersey\tdemocrat\t0\t1\t1\t3\t0\ta news conference\n",
      "11191.json\ttrue\tSays Donald Trump has changed his mind on abortion.\tabortion\tcarly-fiorina\t\tCalifornia\trepublican\t5\t5\t4\t3\t2\tthe first Republican presidential debate\n",
      "10315.json\tmostly-true\tIt has been many years, if ever, since an inmate has completed his or her high school diploma while incarcerated in a state correctional facility for adults.\tcorrections-and-updates,criminal-justice,public-safety\tlc-buster-evans\tAssistant Commissioner\tGeorgia\tnone\t0\t0\t0\t1\t0\tin a press release\n",
      "1964.json\tmostly-true\tSays U.S. Rep. Michael McCaul is the sixth-richest person in Congress.\tcandidates-biography\tted-ankrum\tRetired\tTexas\tdemocrat\t0\t1\t0\t1\t0\ta speech\n",
      "624.json\ttrue\tMcCain is raising campaign cash with one of (Jack) Abramoff's closest business partners: scandal-plagued conservative activist Ralph Reed.\"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "all_aggregate = []\n",
    "for i,obs in enumerate(test[\"text\"].tolist()):\n",
    "    if(device == \"cuda\"):\n",
    "        obs_pt = pipeline.tokenizer(obs, return_tensors=\"pt\")['input_ids'].cuda()\n",
    "    else:\n",
    "        obs_pt = pipeline.tokenizer(obs, return_tensors=\"pt\")['input_ids'].cpu()\n",
    "    attr = FeatureAblationText(forward)\n",
    "\n",
    "    exp = attr.get_attributions([obs_pt])\n",
    "    tokens = pipeline.tokenizer.convert_ids_to_tokens(obs_pt[0])\n",
    "\n",
    "\n",
    "    doc = NER(obs)\n",
    "    tokens_clear = [s.replace(\"Ġ\", \"\") for s in tokens]\n",
    "    tokens_clear = tokens_clear[1:len(tokens_clear)-1]\n",
    "    spacy_token_to_our_tokens = TokenAggregate.generate_aggregate_list(doc,exp,tokens_clear,tokens)\n",
    "    if spacy_token_to_our_tokens is not False:\n",
    "        all_aggregate = all_aggregate+spacy_token_to_our_tokens\n",
    "    else:\n",
    "        print(i)\n",
    "        print(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aggregate in all_aggregate:\n",
    "    if(len(aggregate.spacy_token)>1):\n",
    "        print(aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_ner_types = set()\n",
    "\n",
    "for aggregate in all_aggregate:\n",
    "    for ner in aggregate.NERs:\n",
    "        all_ner_types.add(ner)\n",
    "\n",
    "exps = {}\n",
    "\n",
    "for ner_type in all_ner_types:\n",
    "    exps[ner_type] = []\n",
    "\n",
    "for aggregate in all_aggregate:\n",
    "    value = np.max(aggregate.model_exp)\n",
    "\n",
    "    true_ners = [element for element in aggregate.NERs if element != '']\n",
    "    if(len(true_ners)==0):\n",
    "        exps[''].append(value)\n",
    "    else:\n",
    "        for ner in true_ners:\n",
    "            exps[ner].append(value)\n",
    "\n",
    "\n",
    "sns.boxplot(exps)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
